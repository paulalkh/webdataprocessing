import numpy as np
import spacy
import requests
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.metrics.pairwise import cosine_similarity

nlp = spacy.load("en_core_web_md")

def preprocess_and_vectorize(text, nlp):
    doc = nlp(text)
    return [(ent.text, ent.label_) for ent in doc.ents]

def vector_similarity(v1, v2):
    vectorizer = CountVectorizer()
    v1_text = ' '.join([str(v[0]) for v in v1 if v]) if v1 and isinstance(v1[0], tuple) else ' '.join(map(str, v1))
    v2_text = ' '.join([str(v[0]) for v in v2 if v]) if v2 and isinstance(v2[0], tuple) else ' '.join(map(str, v2))
    
    vectors = vectorizer.fit_transform([v1_text, v2_text])
    similarity = cosine_similarity(vectors)[0][1]
    
    return similarity

def validate_answer_correctness(extracted_answer, entities, nlp):
    query_terms = [entity[0].replace(' ', '_') for entity in entities]
    query = ' '.join(query_terms)

    wdqs_endpoint = "https://query.wikidata.org/sparql"
    sparql_query = f"""
    SELECT ?item ?itemLabel ?itemDescription WHERE {{
      ?item ?label "{query}"@en.
      SERVICE wikibase:label {{ bd:serviceParam wikibase:language "[AUTO_LANGUAGE],en". }}
    }}
    """

    headers = {
        "User-Agent": "YourUserAgent",
        "Accept": "application/json",
    }

    print(f"Sending SPARQL query to Wikidata: {sparql_query}")

    response = requests.get(wdqs_endpoint, params={"query": sparql_query, "format": "json"}, headers=headers)
    response_json = response.json()

    if "results" in response_json and "bindings" in response_json["results"]:
        wikidata_entities = [
            {
                "id": binding["item"]["value"],
                "label": binding["itemLabel"]["value"],
                "description": binding.get("itemDescription", {}).get("value", "")
            }
            for binding in response_json["results"]["bindings"]
        ]

        extracted_entities = preprocess_and_vectorize(' '.join([item[0] for item in extracted_answer]), nlp)
        print(extracted_entities)
        # Find the closest match using vector similarity
        similarity_scores = [
            vector_similarity(extracted_entities, preprocess_and_vectorize(entity["label"], nlp))
            for entity in wikidata_entities
            if preprocess_and_vectorize(entity["label"], nlp)
        ]

        if similarity_scores:
            max_similarity = max(similarity_scores)
            
            print(f"Similarity Scores: {similarity_scores}")
            print(f"Max Similarity Score: {max_similarity}")

            if max_similarity > 0.7:  # You can adjust the threshold as needed
                print("Correct: 'yes'")
                return 'yes', wikidata_entities

    print("Correct: 'no'")
    return 'no', []

# Test the correctness check using Wikidata
question = "What is the capital of Nicaragua?"
answer = "Managua is the capital of Nicaragua. The population of Managua is 1.3 million people."

question_entities = preprocess_and_vectorize(question, nlp)
extracted_answer = preprocess_and_vectorize(answer, nlp)

print(f"Question Entities: {question_entities}")
print(f"Extracted Answer: {extracted_answer}")

validation_result, matched_entities = validate_answer_correctness(extracted_answer, question_entities, nlp)

print(f"Is the extracted answer correct? {validation_result}")

if matched_entities:
    print("Matched Wikidata Entities:")
    for entity in matched_entities:
        print(f"ID: {entity['id']}, Label: {entity['label']}, Description: {entity['description']}")
